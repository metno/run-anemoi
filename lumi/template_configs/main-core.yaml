defaults:
- data: zarr
- dataloader: native_grid
- datamodule: single
- diagnostics: evaluation
- hardware: slurm
- graph: limited_area # --> we can create the graph in another step: uncomment this and only add minimum needed under 'graph'
- model: graphtransformer
- training: default 
- override hydra/hydra_logging: disabled
- override hydra/job_logging: disabled
- _self_

config_validation: False

hydra:  
  output_subdir: null  
  run:  
    dir: .

data:
  frequency: 3h
  timestep: 3h
  forcing:
  - h
  - Pair             
  - Qair             
  - Tair   
  - Uwind
  - Vwind              
  #- cloud # available from 2017       
  - cos_julian_day   
  - cos_latitude     
  - cos_local_time   
  - cos_longitude    
  - f                
  - insolation       
  - rain             
  - river_binary_mask
  - sin_julian_day   
  - sin_latitude   
  - sin_local_time
  - sin_longitude
  - sea_mask

  diagnostic: []
    
  processors:
    imputer:
      #_target_: anemoi.models.preprocessing.imputer.InputImputer #ConstantImputer
      #config: 
      #  default: "none"
      #  minimum: []
      #  none: []
      #  mean:  # TODO specify "none" for the other variables?
      #  - temperature_0
      #  - salinity_0
      #  - zeta
      
      # TODO: would like to use InputImputer but we get this error:
      # hydra.errors.InstantiationException: Error in call to target 'anemoi.models.preprocessing.imputer.InputImputer':
      # TypeError("Statistics <class 'omegaconf.dictconfig.DictConfig'> is optional and not a dictionary")

      _target_: anemoi.models.preprocessing.imputer.ConstantImputer
      config:
        default: "none"
        0:
        - temperature_0
        - salinity_0
        - zeta
  
    normalizer:
      _target_: anemoi.models.preprocessing.normalizer.InputNormalizer

      config:
        default: "mean-std"
        std: []
        min-max:
        max: # Data is normalised by dividing by the max value (, so the ‘zero’ point and the proportional distance from this point is retained)
        - h
        none:
        - cos_julian_day   
        - cos_latitude     
        - cos_local_time   
        - cos_longitude  
        - insolation           
        - sin_julian_day   
        - sin_latitude   
        - sin_local_time
        - sin_longitude
        #- cloud # available from 2017
        - river_binary_mask
        - sea_mask
      
dataloader:
  num_workers:
    training: 2
    validation: 2
    test: 2
  batch_size: 
    training: 1 # has to be 1 for model-paralell
    validation: 1
    test: 1

  limit_batches:
    training: null
    validation: null
    test: 20 # TODO used to have null here
  
  dataset:
    cutout:
    - dataset:
        join: # joining all main and forcing datasets from 2017-2024
          - concat: 
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main12}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main13}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main14}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main15}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main16}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main17}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main18}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main19}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main20}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main21}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main22}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main23}
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main24}
            frequency: ${data.frequency}
          - concat:
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force12}
              start: 2012
              end: 2012
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force13}
              start: 2013
              end: 2013
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force14}
              start: 2014
              end: 2014
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force15}
              start: 2015
              end: 2015
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force16}
              start: 2016
              end: 2016
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force17}
              start: 2017
              end: 2017
              drop: cloud
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force18}
              start: 2018
              end: 2018
              drop: cloud
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force19}
              start: 2019
              end: 2019
              drop: cloud
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force20}
              start: 2020
              end: 2020
              drop: cloud
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force21}
              start: 2021
              end: 2021
              drop: cloud
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force22}
              start: 2022
              end: 2022
              drop: cloud
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force23}
              start: 2023
              end: 2023
              drop: cloud
            - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force24}
              start: 2024
              end: 2024
              drop: cloud
            frequency: ${data.frequency}
        adjust: ["start", "end"]
      trim_edge: 10
    - join:
      - concat: 
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main12}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main13}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main14}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main15}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main16}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main17}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main18}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main19}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main20}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main21}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main22}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main23}
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_main24}
        frequency: ${data.frequency}
      - concat:
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force12}
          start: 2012
          end: 2012
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force13}
          start: 2013
          end: 2013
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force14}
          start: 2014
          end: 2014
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force15}
          start: 2015
          end: 2015
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force16}
          start: 2016
          end: 2016
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force17}
          start: 2017
          end: 2017
          drop: cloud
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force18}
          start: 2018
          end: 2018
          drop: cloud
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force19}
          start: 2019
          end: 2019
          drop: cloud
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force20}
          start: 2020
          end: 2020
          drop: cloud
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force21}
          start: 2021
          end: 2021
          drop: cloud
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force22}
          start: 2022
          end: 2022
          drop: cloud
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force23}
          start: 2023
          end: 2023
          drop: cloud
        - dataset: ${hardware.paths.data}/${hardware.files.dataset.dataset_force24}
          start: 2024
          end: 2024
          drop: cloud
        frequency: ${data.frequency}
      adjust: ["start", "end"]
    adjust: all

  training:
    dataset: ${dataloader.dataset} 
    start: 2012-01-01
    end: 2022-06-30
    drop: ['vbar_northward', 'ubar_eastward', 'Uwind_eastward', 'Vwind_northward']

  validation_rollout: 1 # TODO change later

  validation:
    dataset: ${dataloader.dataset}
    start: 2022-07-01
    end: 2023-09-30
    drop: ['vbar_northward', 'ubar_eastward', 'Uwind_eastward', 'Vwind_northward']
  test:
    dataset: ${dataloader.dataset}
    start: 2023-10-01
    end: 2024-12-31
    drop: ['vbar_northward', 'ubar_eastward', 'Uwind_eastward', 'Vwind_northward']

diagnostics:
  plot: 
    callbacks: []
  log:
    mlflow:
      enabled: True
      offline: True
      authentication: True
      tracking_uri: https://mlflow.ecmwf.int
      experiment_name: 'metno-fou'
      run_name: 'lam-graph-test' #change this
    wandb: 
      entity: null
  print_memory_summary: True

hardware:
  paths:
    data: /pfs/lustrep3/scratch/project_465001902/datasets/
    output: /pfs/lustrep3/scratch/project_465001902/experiments/lam-graph-test/
    graph: /pfs/lustrep3/scratch/project_465001902/graphs/
  files:
    dataset:
      dataset_main12: norkystv3_hindcast_2012_surface.zarr
      dataset_main13: norkystv3_hindcast_2013_surface.zarr
      dataset_main14: norkystv3_hindcast_2014_surface.zarr
      dataset_main15: norkystv3_hindcast_2015_surface.zarr
      dataset_main16: norkystv3_hindcast_2016_surface.zarr
      dataset_main17: norkystv3_hindcast_2017_surface.zarr
      dataset_main18: norkystv3_hindcast_2018_surface.zarr
      dataset_main19: norkystv3_hindcast_2019_surface.zarr
      dataset_main20: norkystv3_hindcast_2020_surface.zarr
      dataset_main21: norkystv3_hindcast_2021_surface.zarr
      dataset_main22: norkystv3_hindcast_2022_surface.zarr
      dataset_main23: norkystv3_hindcast_2023_surface.zarr
      dataset_main24: norkystv3_hindcast_2024_surface.zarr
      dataset_force12: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2012.zarr
      dataset_force13: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2013.zarr
      dataset_force14: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2014.zarr
      dataset_force15: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2015.zarr
      dataset_force16: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2016.zarr
      dataset_force17: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2017.zarr
      dataset_force18: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2018.zarr
      dataset_force19: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2019.zarr
      dataset_force20: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2020.zarr
      dataset_force21: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2021.zarr
      dataset_force22: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2022.zarr
      dataset_force23: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2023.zarr
      dataset_force24: norkystv3_forcing_zarr/forcing_norkystv3_hindcast_2024.zarr
    graph: trim_edge_10_res_10.pt
    warm_start: null #specific checkpoint to start from, defaults to last.ckpt

  #num_gpus_per_node: 1 # using slurm.yaml so these two are now read from SLURM env vars set in lumi_jobscript.sh
  #num_nodes: 1
  num_gpus_per_model: 4 # TODO: 8 better? This is so-called "model-paralell"

graph:
  overwrite: False # only create/re-generate the graph with True
  nodes:
    data:
      hidden:
        node_builder:
          resolution: 10
  attributes:
    nodes:
      area_weight:
        _target_: anemoi.graphs.nodes.attributes.UniformWeights # default is PlanarAreaWeights

model: 
  num_channels: 256 #1024 #TODO: this number dont have to be so large when have few vars, use higher nr when have more layers vars. Test to see if higher than 256 gives added value. Or higher when have less mesh points (remove land)
  trainable_parameters:
    data: 0
    hidden: 0
    data2hidden: 0
    hidden2data: 0
    hidden2hidden: 0 # GNN and GraphTransformer Processor only
  bounding: #These are applied in order
    - _target_: anemoi.models.layers.bounding.ReluBounding #[0, infinity)
      variables: [salinity_0]
    - _target_: anemoi.models.layers.bounding.NormalizedReluBounding
      variables: [temperature_0]
      min_val: [-2]
      normalizer: ['mean-std']

training:
  # This section is to avoid using variable_loss_scaling 
  training_loss:
    _target_: anemoi.training.losses.mse.WeightedMSELoss
    scalars: ['loss_weights_mask'] # which means removing 'variable' thus not using variable_loss_scaling which has hardcoded vars under "pl" and "scf"
  
  scale_validation_metrics:
    scalars_to_apply: []
    metrics:
      - 'all'
  
  metrics: # this used to list only a few variables
  - 'all'  

  #TODO rollout is here

  run_id: null #path to store the experiment in with output_base as root, null for random name, =fork_run_id to continue training in the same folder.
  fork_run_id: null #path to the experiment to fork from with output_base as root
  load_weights_only: False #loads entire model if False, loads only weights if True
  
  max_epochs: null
  max_steps: 150000
  lr:
    warmup: 1000 # number of warmup iterations
    rate: 6.25e-5 #local_lr
    iterations: ${training.max_steps} # NOTE: When max_epochs < max_steps, scheduler will run for max_steps
    min: 3e-7 #Not scaled by #GPU

