defaults:
- data: zarr
- dataloader: native_grid
- datamodule: single
- diagnostics: evaluation
- hardware: example
- graph: multi_scale #TODO consider using limited_area & new graph --> then use training.lam.yaml below
- model: graphtransformer
- training: default #lam --> implies using scalers/lam.yaml, then add '!limited_area_mask' to training.training_loss.scalers 
- override hydra/hydra_logging: disabled
- override hydra/job_logging: disabled
- _self_

config_validation: False

hydra:  
  output_subdir: null  
  run:  
    dir: .

data:
  frequency: 3h
  timestep: 3h
  forcing:
  - h
  - Uwind_eastward
  - Vwind_northward
  #- "cos_latitude"
  #- "cos_longitude"
  #- "sin_latitude"t 
  #- "sin_longitude"
  #- "cos_julian_day"
  #- "cos_local_time"
  #- "sin_julian_day"
  #- "sin_local_time"
  #- "insolation"

  diagnostic: []

  normalizer:
    default: "mean-std"
    std: []
    min-max:
    max: []
    none: []
    #- "h"
    #- "Uwind_eastward"
    #- "Vwind_northward" # TODO add normalization later
    #- "cos_latitude"
    #- "cos_longitude"
    #- "sin_latitude"
    #- "sin_longitude"
    #- "cos_julian_day"
    #- "cos_local_time"
    #- "sin_julian_day"
    #- "sin_local_time"
    #- "insolation"

  imputer:
    #legge til flere variabler her n√•r vi har det
    default: "none"
    mean: # TODO fix this!
    - temperature_0
    - salinity_0
    - zeta
    - Uwind_eastward
    - Vwind_northward
    maximum: []
    minimum: []
    
  processors:
    imputer:
      _target_: anemoi.models.preprocessing.imputer.InputImputer #ConstantImputer
      #_convert_: 'all'
      config: ${data.imputer}

dataloader:
  num_workers:
    training: 2 #TODO set to 8 later
    validation: 2
    test: 2
  batch_size:
    training: 2
    validation: 4
    test: 4

  limit_batches:
    training: null
    validation: null
    test: 20 # TODO used to have null here
  
  dataset: ${hardware.paths.data}/${hardware.files.dataset}

  training:
    dataset: ${dataloader.dataset} 
    start: 2023-01-01
    end: 2023-01-03
    drop: ['vbar_northward', 'ubar_eastward']

  validation_rollout: 1 # TODO change later

  validation:
    dataset: ${dataloader.dataset}
    start: 2023-02-01
    end: 2023-02-28
    drop: ['vbar_northward', 'ubar_eastward']
  test:
    dataset: ${dataloader.dataset}
    start: 2023-04-01
    end: 2023-04-03
    drop: ['vbar_northward', 'ubar_eastward']

diagnostics:
  plot: 
    callbacks: []
  log:
    mlflow:
      enabled: True
      offline: True
      authentication: True
      tracking_uri: https://mlflow.ecmwf.int
      experiment_name: 'metno-fou'
      run_name: 'hindcast-surface-env-test' #change this
    wandb: 
      entity: null
  print_memory_summary: True

hardware:
  paths:
    data: /pfs/lustrep3/scratch/project_465001902/datasets/
    output: /pfs/lustrep3/scratch/project_465001902/experiments/hindcast-surface-env-test/
    graph: /pfs/lustrep3/scratch/project_465001902/graphs/
  files:
    dataset: norkystv3_hindcast_2023_surface.zarr
    graph: graph-17-12-res7.pt # TODO update graph & resolution
    warm_start: null #specific checkpoint to start from, defaults to last.ckpt

  num_gpus_per_node: 1 # TODO change later & use slurm.yaml not example.yaml
  num_nodes: 1
  num_gpus_per_model: 1
  accelerator: auto

graph:
  overwrite: False

model: 
  num_channels: 1024
  trainable_parameters:
    data: 0
    hidden: 0
    data2hidden: 0
    hidden2data: 0
    hidden2hidden: 0 # GNN and GraphTransformer Processor only
  bounding: #These are applied in order
    - _target_: anemoi.models.layers.bounding.ReluBounding #[0, infinity)
      variables: [salinity_0]
    - _target_: anemoi.models.layers.bounding.NormalizedReluBounding
      variables: [temperature_0]
      min_val: [-2]
      normalizer: ['mean-std']

training:
  training_loss:
    scalers: ['*', '!pressure_level', '!general_variable', '!stdev_tendency', '!var_tendency']

  variable_groups:
    default: []
    pl: []

  metrics: []

  #TODO rollout is here

  run_id: null #path to store the experiment in with output_base as root, null for random name, =fork_run_id to continue training in the same folder.
  fork_run_id: null #path to the experiment to fork from with output_base as root
  load_weights_only: False #loads entire model if False, loads only weights if True
  
  max_epochs: null
  max_steps: 150000
  lr:
    warmup: 1000 # number of warmup iterations
    rate: 6.25e-5 #local_lr
    iterations: ${training.max_steps} # NOTE: When max_epochs < max_steps, scheduler will run for max_steps
    min: 3e-7 #Not scaled by #GPU

